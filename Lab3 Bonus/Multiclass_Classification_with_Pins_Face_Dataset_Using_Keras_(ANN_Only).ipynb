{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import kagglehub\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Configuration\n",
        "IMG_SIZE = 100\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50"
      ],
      "metadata": {
        "id": "BCQ4nwDwbCDI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "def save_processed_data(images, labels, label_map, save_dir='processed_data'):\n",
        "    \"\"\"\n",
        "    Save the processed images, labels, and label map to disk\n",
        "\n",
        "    Args:\n",
        "        images (np.array): Processed face images\n",
        "        labels (np.array): Corresponding labels\n",
        "        label_map (dict): Mapping of label indices to person names\n",
        "        save_dir (str): Directory to save the processed data\n",
        "    \"\"\"\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Generate timestamp for the filename\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    # Save the data\n",
        "    data = {\n",
        "        'images': images,\n",
        "        'labels': labels,\n",
        "        'label_map': label_map,\n",
        "        'timestamp': timestamp\n",
        "    }\n",
        "\n",
        "    filepath = os.path.join(save_dir, f'processed_data_{timestamp}.pkl')\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "    print(f\"Data saved to: {filepath}\")\n",
        "\n",
        "    # Save metadata for easy loading\n",
        "    metadata = {\n",
        "        'latest_file': filepath,\n",
        "        'n_images': len(images),\n",
        "        'n_classes': len(label_map)\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(save_dir, 'metadata.pkl'), 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "def load_processed_data(filepath=None, data_dir='processed_data'):\n",
        "    \"\"\"\n",
        "    Load previously processed data from disk\n",
        "\n",
        "    Args:\n",
        "        filepath (str, optional): Specific file to load. If None, loads the latest file\n",
        "        data_dir (str): Directory containing the processed data\n",
        "\n",
        "    Returns:\n",
        "        tuple: (images, labels, label_map)\n",
        "    \"\"\"\n",
        "    if filepath is None:\n",
        "        # Load metadata to get the latest file\n",
        "        metadata_path = os.path.join(data_dir, 'metadata.pkl')\n",
        "        if not os.path.exists(metadata_path):\n",
        "            raise FileNotFoundError(\"No metadata file found. Please process the data first.\")\n",
        "\n",
        "        with open(metadata_path, 'rb') as f:\n",
        "            metadata = pickle.load(f)\n",
        "        filepath = metadata['latest_file']\n",
        "\n",
        "    if not os.path.exists(filepath):\n",
        "        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n",
        "\n",
        "    print(f\"Loading data from: {filepath}\")\n",
        "    with open(filepath, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "    return data['images'], data['labels'], data['label_map']\n",
        "\n",
        "def load_and_preprocess_data(save_dir='processed_data', force_reprocess=False):\n",
        "    \"\"\"\n",
        "    Load and preprocess images from the dataset using kagglehub with caching\n",
        "\n",
        "    Args:\n",
        "        save_dir (str): Directory to save/load processed data\n",
        "        force_reprocess (bool): If True, reprocess data even if cached version exists\n",
        "\n",
        "    Returns:\n",
        "        tuple: (images, labels, label_map)\n",
        "    \"\"\"\n",
        "    # Check for existing processed data\n",
        "    if not force_reprocess and os.path.exists(save_dir):\n",
        "        try:\n",
        "            return load_processed_data(data_dir=save_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading cached data: {e}\")\n",
        "            print(\"Proceeding with data processing...\")\n",
        "\n",
        "    # Download dataset\n",
        "    print(\"Downloading dataset...\")\n",
        "    dataset_path = kagglehub.dataset_download(\"hereisburak/pins-face-recognition\")\n",
        "    print(\"Dataset downloaded to:\", dataset_path)\n",
        "\n",
        "    # Find the main dataset directory\n",
        "    main_dir = None\n",
        "    for item in os.listdir(dataset_path):\n",
        "        if \"105_classes_pins_dataset\" in item:\n",
        "            main_dir = os.path.join(dataset_path, item)\n",
        "            break\n",
        "\n",
        "    if main_dir is None:\n",
        "        raise Exception(\"Could not find the main dataset directory\")\n",
        "\n",
        "    # Initialize lists to store images and labels\n",
        "    images = []\n",
        "    labels = []\n",
        "    label_map = {}\n",
        "    current_label = 0\n",
        "\n",
        "    # Load face detection model\n",
        "    face_net = cv2.dnn.readNetFromCaffe(\n",
        "        'deploy.prototxt',\n",
        "        'res10_300x300_ssd_iter_140000.caffemodel'\n",
        "    )\n",
        "\n",
        "    # Walk through the person directories (prefixed with \"pins_\")\n",
        "    for person_dir in sorted(os.listdir(main_dir)):\n",
        "        if person_dir.startswith('pins_'):\n",
        "            person_name = person_dir[5:]  # Remove 'pins_' prefix\n",
        "            person_path = os.path.join(main_dir, person_dir)\n",
        "\n",
        "            print(f\"Processing images for: {person_name}\")\n",
        "            label_map[current_label] = person_name\n",
        "\n",
        "            for img_name in os.listdir(person_path):\n",
        "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(person_path, img_name)\n",
        "\n",
        "                    # Read and preprocess image\n",
        "                    img = cv2.imread(img_path)\n",
        "                    if img is None:\n",
        "                        print(f\"Warning: Could not read image {img_path}\")\n",
        "                        continue\n",
        "\n",
        "                    # Face detection\n",
        "                    (h, w) = img.shape[:2]\n",
        "                    blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), 1.0,\n",
        "                                               (300, 300), (104.0, 177.0, 123.0))\n",
        "                    face_net.setInput(blob)\n",
        "                    detections = face_net.forward()\n",
        "\n",
        "                    # Get the face with highest confidence\n",
        "                    max_conf = 0\n",
        "                    max_face = None\n",
        "\n",
        "                    for i in range(0, detections.shape[2]):\n",
        "                        confidence = detections[0, 0, i, 2]\n",
        "                        if confidence > 0.5:  # Minimum confidence threshold\n",
        "                            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "                            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "                            # Ensure coordinates are within image bounds\n",
        "                            startX = max(0, startX)\n",
        "                            startY = max(0, startY)\n",
        "                            endX = min(w, endX)\n",
        "                            endY = min(h, endY)\n",
        "\n",
        "                            # Extract and preprocess face\n",
        "                            face = img[startY:endY, startX:endX]\n",
        "                            if face.size == 0:\n",
        "                                continue\n",
        "\n",
        "                            face = cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n",
        "                            if confidence > max_conf:\n",
        "                                max_conf = confidence\n",
        "                                max_face = face\n",
        "\n",
        "                    if max_face is not None:\n",
        "                        # Normalize pixel values\n",
        "                        max_face = max_face.astype('float32') / 255.0\n",
        "                        images.append(max_face)\n",
        "                        labels.append(current_label)\n",
        "\n",
        "            print(f\"Processed {len([l for l in labels if l == current_label])} images for {person_name}\")\n",
        "            current_label += 1\n",
        "\n",
        "    if len(images) == 0:\n",
        "        raise Exception(\"No images were successfully processed\")\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Save the processed data\n",
        "    save_processed_data(images, labels, label_map, save_dir)\n",
        "\n",
        "    return images, labels, label_map"
      ],
      "metadata": {
        "id": "FzJnRqPwa8GD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "print(\"Loading and preprocessing data...\")\n",
        "X, y, label_map = load_and_preprocess_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIab2JMebeRi",
        "outputId": "771c23a5-3934-4742-bf1c-0c9ac14e4f17"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing data...\n",
            "Loading data from: processed_data/processed_data_20241027_102543.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Create an enhanced ANN model architecture with advanced techniques for better accuracy\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        # Input preprocessing\n",
        "        Flatten(input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        # First dense block\n",
        "        Dense(2048, kernel_initializer='he_uniform'),\n",
        "        BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        # Second dense block with residual connection\n",
        "        Dense(1024, kernel_initializer='he_uniform'),\n",
        "        BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        # Third dense block\n",
        "        Dense(512, kernel_initializer='he_uniform'),\n",
        "        BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Feature extraction block\n",
        "        Dense(256, kernel_initializer='he_uniform'),\n",
        "        BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Deep features block\n",
        "        Dense(128, kernel_initializer='he_uniform'),\n",
        "        BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        # Classification head\n",
        "        Dense(num_classes, activation='softmax',\n",
        "              kernel_initializer='glorot_uniform',\n",
        "              kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "    ])\n",
        "\n",
        "    # Custom learning rate schedule\n",
        "    initial_learning_rate = 0.001\n",
        "    decay_steps = 1000\n",
        "    decay_rate = 0.9\n",
        "    learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate, decay_steps, decay_rate\n",
        "    )\n",
        "\n",
        "    # Custom optimizer with parameters\n",
        "    optimizer = tf.keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate_schedule,\n",
        "        weight_decay=0.001,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-07\n",
        "    )\n",
        "\n",
        "    # Compile with custom settings\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            tf.keras.metrics.SparseCategoricalAccuracy(name='categorical_accuracy'),\n",
        "            tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_accuracy')\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_model(model, X_train, y_train, X_test, y_test, epochs=50, batch_size=32):\n",
        "    \"\"\"\n",
        "    Enhanced training function with advanced callbacks and monitoring\n",
        "    \"\"\"\n",
        "    # Define initial_learning_rate here\n",
        "    initial_learning_rate = 0.001\n",
        "\n",
        "    # Enhanced data augmentation\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.1,\n",
        "        fill_mode='nearest',\n",
        "        brightness_range=[0.8, 1.2],\n",
        "        preprocessing_function=tf.keras.applications.imagenet_utils.preprocess_input\n",
        "    )\n",
        "\n",
        "    # Advanced callbacks\n",
        "    callbacks = [\n",
        "        # Early stopping with restoration\n",
        "        EarlyStopping(\n",
        "            monitor='val_categorical_accuracy',\n",
        "            patience=15,\n",
        "            restore_best_weights=True,\n",
        "            mode='max'\n",
        "        ),\n",
        "\n",
        "        # Adaptive learning rate reduction\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_categorical_accuracy',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-6,\n",
        "            mode='max',\n",
        "            verbose=1\n",
        "        ),\n",
        "\n",
        "        # Model checkpoint\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'best_model.keras',  # Change the filepath to end with .keras\n",
        "            monitor='val_categorical_accuracy',\n",
        "            mode='max',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        " # Train with advanced settings\n",
        "    history = model.fit(\n",
        "        datagen.flow(X_train, y_train, batch_size=batch_size),\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks,\n",
        "        # Removed 'workers' and 'use_multiprocessing' arguments\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    return history, model\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plot training and validation metrics\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax1.plot(history.history['accuracy'])\n",
        "    ax1.plot(history.history['val_accuracy'])\n",
        "    ax1.set_title('Model Accuracy')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend(['Train', 'Validation'])\n",
        "\n",
        "    # Plot loss\n",
        "    ax2.plot(history.history['loss'])\n",
        "    ax2.plot(history.history['val_loss'])\n",
        "    ax2.set_title('Model Loss')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend(['Train', 'Validation'])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "sxTOGFj1H_o7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the preprocessed data\n",
        "X, y, label_map = load_processed_data()\n",
        "\n",
        "print(f\"Dataset summary:\")\n",
        "print(f\"Total number of images: {len(X)}\")\n",
        "print(f\"Number of classes: {len(label_map)}\")\n",
        "print(f\"Images per class:\")\n",
        "for label, name in label_map.items():\n",
        "    count = np.sum(y == label)\n",
        "    print(f\"{name}: {count} images\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "\n",
        "# Create data generator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Create and train model with enhanced settings\n",
        "model = create_model(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=len(label_map))\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Train with enhanced function\n",
        "history, trained_model = train_model(\n",
        "    model, X_train, y_train, X_test, y_test,\n",
        "    epochs=50,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Evaluate model\n",
        "print(\"\\nEvaluating model...\")\n",
        "test_loss, test_accuracy, cat_accuracy, top_5_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n",
        "print(f\"Top-5 accuracy: {top_5_accuracy*100:.2f}%\")\n",
        "\n",
        "\n",
        "# Generate predictions and classification report\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes,\n",
        "                          target_names=[label_map[i] for i in range(len(label_map))]))\n",
        "\n",
        "# Plot training history\n",
        "plot_training_history(history)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 8))\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=list(label_map.values()), yticklabels=list(label_map.values()))\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N3kkhGDoLCpC",
        "outputId": "cc827e51-e659-443a-af23-8ea4ef971ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from: processed_data/processed_data_20241027_102543.pkl\n",
            "Dataset summary:\n",
            "Total number of images: 17533\n",
            "Number of classes: 105\n",
            "Images per class:\n",
            "Adriana Lima: 213 images\n",
            "Alex Lawther: 152 images\n",
            "Alexandra Daddario: 225 images\n",
            "Alvaro Morte: 139 images\n",
            "Amanda Crew: 117 images\n",
            "Andy Samberg: 196 images\n",
            "Anne Hathaway: 203 images\n",
            "Anthony Mackie: 124 images\n",
            "Avril Lavigne: 162 images\n",
            "Ben Affleck: 126 images\n",
            "Bill Gates: 122 images\n",
            "Bobby Morley: 138 images\n",
            "Brenton Thwaites: 209 images\n",
            "Brian J. Smith: 102 images\n",
            "Brie Larson: 169 images\n",
            "Chris Evans: 166 images\n",
            "Chris Hemsworth: 159 images\n",
            "Chris Pratt: 176 images\n",
            "Christian Bale: 154 images\n",
            "Cristiano Ronaldo: 98 images\n",
            "Danielle Panabaker: 181 images\n",
            "Dominic Purcell: 146 images\n",
            "Dwayne Johnson: 141 images\n",
            "Eliza Taylor: 162 images\n",
            "Elizabeth Lail: 158 images\n",
            "Emilia Clarke: 210 images\n",
            "Emma Stone: 139 images\n",
            "Emma Watson: 211 images\n",
            "Gwyneth Paltrow: 187 images\n",
            "Henry Cavil: 195 images\n",
            "Hugh Jackman: 179 images\n",
            "Inbar Lavi: 127 images\n",
            "Irina Shayk: 156 images\n",
            "Jake Mcdorman: 159 images\n",
            "Jason Momoa: 184 images\n",
            "Jennifer Lawrence: 180 images\n",
            "Jeremy Renner: 167 images\n",
            "Jessica Barden: 141 images\n",
            "Jimmy Fallon: 113 images\n",
            "Johnny Depp: 182 images\n",
            "Josh Radnor: 117 images\n",
            "Katharine Mcphee: 177 images\n",
            "Katherine Langford: 226 images\n",
            "Keanu Reeves: 160 images\n",
            "Krysten Ritter: 171 images\n",
            "Leonardo DiCaprio: 237 images\n",
            "Lili Reinhart: 150 images\n",
            "Lindsey Morgan: 169 images\n",
            "Lionel Messi: 86 images\n",
            "Logan Lerman: 212 images\n",
            "Madelaine Petsch: 191 images\n",
            "Maisie Williams: 193 images\n",
            "Maria Pedraza: 122 images\n",
            "Marie Avgeropoulos: 161 images\n",
            "Mark Ruffalo: 178 images\n",
            "Mark Zuckerberg: 95 images\n",
            "Megan Fox: 209 images\n",
            "Miley Cyrus: 178 images\n",
            "Millie Bobby Brown: 191 images\n",
            "Morena Baccarin: 175 images\n",
            "Morgan Freeman: 105 images\n",
            "Nadia Hilker: 133 images\n",
            "Natalie Dormer: 198 images\n",
            "Natalie Portman: 166 images\n",
            "Neil Patrick Harris: 116 images\n",
            "Pedro Alonso: 125 images\n",
            "Penn Badgley: 171 images\n",
            "Rami Malek: 160 images\n",
            "Rebecca Ferguson: 178 images\n",
            "Richard Harmon: 148 images\n",
            "Rihanna: 133 images\n",
            "Robert De Niro: 156 images\n",
            "Robert Downey Jr: 233 images\n",
            "Sarah Wayne Callies: 159 images\n",
            "Selena Gomez: 186 images\n",
            "Shakira Isabel Mebarak: 154 images\n",
            "Sophie Turner: 204 images\n",
            "Stephen Amell: 159 images\n",
            "Taylor Swift: 131 images\n",
            "Tom Cruise: 192 images\n",
            "Tom Hardy: 198 images\n",
            "Tom Hiddleston: 181 images\n",
            "Tom Holland: 189 images\n",
            "Tuppence Middleton: 133 images\n",
            "Ursula Corbero: 167 images\n",
            "Wentworth Miller: 179 images\n",
            "Zac Efron: 191 images\n",
            "Zendaya: 138 images\n",
            "Zoe Saldana: 186 images\n",
            "alycia dabnem carey: 211 images\n",
            "amber heard: 218 images\n",
            "barack obama: 119 images\n",
            "barbara palvin: 197 images\n",
            "camila mendes: 162 images\n",
            "elizabeth olsen: 221 images\n",
            "ellen page: 188 images\n",
            "elon musk: 135 images\n",
            "gal gadot: 199 images\n",
            "grant gustin: 183 images\n",
            "jeff bezos: 106 images\n",
            "kiernen shipka: 203 images\n",
            "margot robbie: 221 images\n",
            "melissa fumero: 154 images\n",
            "scarlett johansson: 201 images\n",
            "tom ellis: 180 images\n",
            "\n",
            "Training set size: 14026\n",
            "Test set size: 3507\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30000</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30000</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">120,000</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │      <span style=\"color: #00af00; text-decoration-color: #00af00\">61,442,048</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,098,176</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_9                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_10               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_11               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">13,545</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30000\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30000\u001b[0m)               │         \u001b[38;5;34m120,000\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │      \u001b[38;5;34m61,442,048\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │           \u001b[38;5;34m8,192\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_5 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │       \u001b[38;5;34m2,098,176\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │           \u001b[38;5;34m4,096\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_6 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m524,800\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_9                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_7 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m131,328\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_10               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │           \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_8 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_11               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_9 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m105\u001b[0m)                 │          \u001b[38;5;34m13,545\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,378,665</span> (245.59 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m64,378,665\u001b[0m (245.59 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,310,729</span> (245.33 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m64,310,729\u001b[0m (245.33 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,936</span> (265.38 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m67,936\u001b[0m (265.38 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.0102 - categorical_accuracy: 0.0102 - loss: 5.5128 - top_5_accuracy: 0.0516\n",
            "Epoch 1: val_categorical_accuracy improved from -inf to 0.01255, saving model to best_model.keras\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m877s\u001b[0m 2s/step - accuracy: 0.0102 - categorical_accuracy: 0.0102 - loss: 5.5121 - top_5_accuracy: 0.0516 - val_accuracy: 0.0125 - val_categorical_accuracy: 0.0125 - val_loss: 11087.8652 - val_top_5_accuracy: 1.0000 - learning_rate: 9.5480e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.0130 - categorical_accuracy: 0.0130 - loss: 4.7835 - top_5_accuracy: 0.0579\n",
            "Epoch 2: val_categorical_accuracy did not improve from 0.01255\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m864s\u001b[0m 2s/step - accuracy: 0.0130 - categorical_accuracy: 0.0130 - loss: 4.7834 - top_5_accuracy: 0.0579 - val_accuracy: 0.0091 - val_categorical_accuracy: 0.0091 - val_loss: 38948.6641 - val_top_5_accuracy: 1.0000 - learning_rate: 9.1164e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.0096 - categorical_accuracy: 0.0096 - loss: 4.6933 - top_5_accuracy: 0.0547\n",
            "Epoch 3: val_categorical_accuracy did not improve from 0.01255\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m799s\u001b[0m 2s/step - accuracy: 0.0096 - categorical_accuracy: 0.0096 - loss: 4.6933 - top_5_accuracy: 0.0547 - val_accuracy: 0.0091 - val_categorical_accuracy: 0.0091 - val_loss: 26020.5898 - val_top_5_accuracy: 1.0000 - learning_rate: 8.7044e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.0114 - categorical_accuracy: 0.0114 - loss: 4.6644 - top_5_accuracy: 0.0612\n",
            "Epoch 4: val_categorical_accuracy did not improve from 0.01255\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m804s\u001b[0m 2s/step - accuracy: 0.0114 - categorical_accuracy: 0.0114 - loss: 4.6644 - top_5_accuracy: 0.0612 - val_accuracy: 0.0091 - val_categorical_accuracy: 0.0091 - val_loss: 24909.2324 - val_top_5_accuracy: 1.0000 - learning_rate: 8.3109e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.0125 - categorical_accuracy: 0.0125 - loss: 4.6597 - top_5_accuracy: 0.0642\n",
            "Epoch 5: val_categorical_accuracy improved from 0.01255 to 0.01340, saving model to best_model.keras\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m883s\u001b[0m 2s/step - accuracy: 0.0125 - categorical_accuracy: 0.0125 - loss: 4.6597 - top_5_accuracy: 0.0642 - val_accuracy: 0.0134 - val_categorical_accuracy: 0.0134 - val_loss: 32896.6797 - val_top_5_accuracy: 1.0000 - learning_rate: 7.9353e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.0137 - categorical_accuracy: 0.0137 - loss: 4.6540 - top_5_accuracy: 0.0627\n",
            "Epoch 6: val_categorical_accuracy did not improve from 0.01340\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m817s\u001b[0m 2s/step - accuracy: 0.0137 - categorical_accuracy: 0.0137 - loss: 4.6540 - top_5_accuracy: 0.0627 - val_accuracy: 0.0100 - val_categorical_accuracy: 0.0100 - val_loss: 34552.2422 - val_top_5_accuracy: 1.0000 - learning_rate: 7.5766e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m421/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.0113 - categorical_accuracy: 0.0113 - loss: 4.6483 - top_5_accuracy: 0.0622"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c0O5On58d3dK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}