{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import kagglehub\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Configuration\n",
        "IMG_SIZE = 100\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50"
      ],
      "metadata": {
        "id": "BCQ4nwDwbCDI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "def save_processed_data(images, labels, label_map, save_dir='processed_data'):\n",
        "    \"\"\"\n",
        "    Save the processed images, labels, and label map to disk\n",
        "\n",
        "    Args:\n",
        "        images (np.array): Processed face images\n",
        "        labels (np.array): Corresponding labels\n",
        "        label_map (dict): Mapping of label indices to person names\n",
        "        save_dir (str): Directory to save the processed data\n",
        "    \"\"\"\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Generate timestamp for the filename\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    # Save the data\n",
        "    data = {\n",
        "        'images': images,\n",
        "        'labels': labels,\n",
        "        'label_map': label_map,\n",
        "        'timestamp': timestamp\n",
        "    }\n",
        "\n",
        "    filepath = os.path.join(save_dir, f'processed_data_{timestamp}.pkl')\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "    print(f\"Data saved to: {filepath}\")\n",
        "\n",
        "    # Save metadata for easy loading\n",
        "    metadata = {\n",
        "        'latest_file': filepath,\n",
        "        'n_images': len(images),\n",
        "        'n_classes': len(label_map)\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(save_dir, 'metadata.pkl'), 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "def load_processed_data(filepath=None, data_dir='processed_data'):\n",
        "    \"\"\"\n",
        "    Load previously processed data from disk\n",
        "\n",
        "    Args:\n",
        "        filepath (str, optional): Specific file to load. If None, loads the latest file\n",
        "        data_dir (str): Directory containing the processed data\n",
        "\n",
        "    Returns:\n",
        "        tuple: (images, labels, label_map)\n",
        "    \"\"\"\n",
        "    if filepath is None:\n",
        "        # Load metadata to get the latest file\n",
        "        metadata_path = os.path.join(data_dir, 'metadata.pkl')\n",
        "        if not os.path.exists(metadata_path):\n",
        "            raise FileNotFoundError(\"No metadata file found. Please process the data first.\")\n",
        "\n",
        "        with open(metadata_path, 'rb') as f:\n",
        "            metadata = pickle.load(f)\n",
        "        filepath = metadata['latest_file']\n",
        "\n",
        "    if not os.path.exists(filepath):\n",
        "        raise FileNotFoundError(f\"Data file not found: {filepath}\")\n",
        "\n",
        "    print(f\"Loading data from: {filepath}\")\n",
        "    with open(filepath, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "\n",
        "    return data['images'], data['labels'], data['label_map']\n",
        "\n",
        "def load_and_preprocess_data(save_dir='processed_data', force_reprocess=False):\n",
        "    \"\"\"\n",
        "    Load and preprocess images from the dataset using kagglehub with caching\n",
        "\n",
        "    Args:\n",
        "        save_dir (str): Directory to save/load processed data\n",
        "        force_reprocess (bool): If True, reprocess data even if cached version exists\n",
        "\n",
        "    Returns:\n",
        "        tuple: (images, labels, label_map)\n",
        "    \"\"\"\n",
        "    # Check for existing processed data\n",
        "    if not force_reprocess and os.path.exists(save_dir):\n",
        "        try:\n",
        "            return load_processed_data(data_dir=save_dir)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading cached data: {e}\")\n",
        "            print(\"Proceeding with data processing...\")\n",
        "\n",
        "    # Download dataset\n",
        "    print(\"Downloading dataset...\")\n",
        "    dataset_path = kagglehub.dataset_download(\"hereisburak/pins-face-recognition\")\n",
        "    print(\"Dataset downloaded to:\", dataset_path)\n",
        "\n",
        "    # Find the main dataset directory\n",
        "    main_dir = None\n",
        "    for item in os.listdir(dataset_path):\n",
        "        if \"105_classes_pins_dataset\" in item:\n",
        "            main_dir = os.path.join(dataset_path, item)\n",
        "            break\n",
        "\n",
        "    if main_dir is None:\n",
        "        raise Exception(\"Could not find the main dataset directory\")\n",
        "\n",
        "    # Initialize lists to store images and labels\n",
        "    images = []\n",
        "    labels = []\n",
        "    label_map = {}\n",
        "    current_label = 0\n",
        "\n",
        "    # Load face detection model\n",
        "    face_net = cv2.dnn.readNetFromCaffe(\n",
        "        'deploy.prototxt',\n",
        "        'res10_300x300_ssd_iter_140000.caffemodel'\n",
        "    )\n",
        "\n",
        "    # Walk through the person directories (prefixed with \"pins_\")\n",
        "    for person_dir in sorted(os.listdir(main_dir)):\n",
        "        if person_dir.startswith('pins_'):\n",
        "            person_name = person_dir[5:]  # Remove 'pins_' prefix\n",
        "            person_path = os.path.join(main_dir, person_dir)\n",
        "\n",
        "            print(f\"Processing images for: {person_name}\")\n",
        "            label_map[current_label] = person_name\n",
        "\n",
        "            for img_name in os.listdir(person_path):\n",
        "                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(person_path, img_name)\n",
        "\n",
        "                    # Read and preprocess image\n",
        "                    img = cv2.imread(img_path)\n",
        "                    if img is None:\n",
        "                        print(f\"Warning: Could not read image {img_path}\")\n",
        "                        continue\n",
        "\n",
        "                    # Face detection\n",
        "                    (h, w) = img.shape[:2]\n",
        "                    blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), 1.0,\n",
        "                                               (300, 300), (104.0, 177.0, 123.0))\n",
        "                    face_net.setInput(blob)\n",
        "                    detections = face_net.forward()\n",
        "\n",
        "                    # Get the face with highest confidence\n",
        "                    max_conf = 0\n",
        "                    max_face = None\n",
        "\n",
        "                    for i in range(0, detections.shape[2]):\n",
        "                        confidence = detections[0, 0, i, 2]\n",
        "                        if confidence > 0.5:  # Minimum confidence threshold\n",
        "                            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "                            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "                            # Ensure coordinates are within image bounds\n",
        "                            startX = max(0, startX)\n",
        "                            startY = max(0, startY)\n",
        "                            endX = min(w, endX)\n",
        "                            endY = min(h, endY)\n",
        "\n",
        "                            # Extract and preprocess face\n",
        "                            face = img[startY:endY, startX:endX]\n",
        "                            if face.size == 0:\n",
        "                                continue\n",
        "\n",
        "                            face = cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n",
        "                            if confidence > max_conf:\n",
        "                                max_conf = confidence\n",
        "                                max_face = face\n",
        "\n",
        "                    if max_face is not None:\n",
        "                        # Normalize pixel values\n",
        "                        max_face = max_face.astype('float32') / 255.0\n",
        "                        images.append(max_face)\n",
        "                        labels.append(current_label)\n",
        "\n",
        "            print(f\"Processed {len([l for l in labels if l == current_label])} images for {person_name}\")\n",
        "            current_label += 1\n",
        "\n",
        "    if len(images) == 0:\n",
        "        raise Exception(\"No images were successfully processed\")\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Save the processed data\n",
        "    save_processed_data(images, labels, label_map, save_dir)\n",
        "\n",
        "    return images, labels, label_map"
      ],
      "metadata": {
        "id": "FzJnRqPwa8GD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "print(\"Loading and preprocessing data...\")\n",
        "X, y, label_map = load_and_preprocess_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIab2JMebeRi",
        "outputId": "721b5892-aa35-4985-9600-4ff8d85749b1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing data...\n",
            "Downloading dataset...\n",
            "Dataset downloaded to: /root/.cache/kagglehub/datasets/hereisburak/pins-face-recognition/versions/1\n",
            "Processing images for: Adriana Lima\n",
            "Processed 213 images for Adriana Lima\n",
            "Processing images for: Alex Lawther\n",
            "Processed 152 images for Alex Lawther\n",
            "Processing images for: Alexandra Daddario\n",
            "Processed 225 images for Alexandra Daddario\n",
            "Processing images for: Alvaro Morte\n",
            "Processed 139 images for Alvaro Morte\n",
            "Processing images for: Amanda Crew\n",
            "Processed 117 images for Amanda Crew\n",
            "Processing images for: Andy Samberg\n",
            "Processed 196 images for Andy Samberg\n",
            "Processing images for: Anne Hathaway\n",
            "Processed 203 images for Anne Hathaway\n",
            "Processing images for: Anthony Mackie\n",
            "Processed 124 images for Anthony Mackie\n",
            "Processing images for: Avril Lavigne\n",
            "Processed 162 images for Avril Lavigne\n",
            "Processing images for: Ben Affleck\n",
            "Processed 126 images for Ben Affleck\n",
            "Processing images for: Bill Gates\n",
            "Processed 122 images for Bill Gates\n",
            "Processing images for: Bobby Morley\n",
            "Processed 138 images for Bobby Morley\n",
            "Processing images for: Brenton Thwaites\n",
            "Processed 209 images for Brenton Thwaites\n",
            "Processing images for: Brian J. Smith\n",
            "Processed 102 images for Brian J. Smith\n",
            "Processing images for: Brie Larson\n",
            "Processed 169 images for Brie Larson\n",
            "Processing images for: Chris Evans\n",
            "Processed 166 images for Chris Evans\n",
            "Processing images for: Chris Hemsworth\n",
            "Processed 159 images for Chris Hemsworth\n",
            "Processing images for: Chris Pratt\n",
            "Processed 176 images for Chris Pratt\n",
            "Processing images for: Christian Bale\n",
            "Processed 154 images for Christian Bale\n",
            "Processing images for: Cristiano Ronaldo\n",
            "Processed 98 images for Cristiano Ronaldo\n",
            "Processing images for: Danielle Panabaker\n",
            "Processed 181 images for Danielle Panabaker\n",
            "Processing images for: Dominic Purcell\n",
            "Processed 146 images for Dominic Purcell\n",
            "Processing images for: Dwayne Johnson\n",
            "Processed 141 images for Dwayne Johnson\n",
            "Processing images for: Eliza Taylor\n",
            "Processed 162 images for Eliza Taylor\n",
            "Processing images for: Elizabeth Lail\n",
            "Processed 158 images for Elizabeth Lail\n",
            "Processing images for: Emilia Clarke\n",
            "Processed 210 images for Emilia Clarke\n",
            "Processing images for: Emma Stone\n",
            "Processed 139 images for Emma Stone\n",
            "Processing images for: Emma Watson\n",
            "Processed 211 images for Emma Watson\n",
            "Processing images for: Gwyneth Paltrow\n",
            "Processed 187 images for Gwyneth Paltrow\n",
            "Processing images for: Henry Cavil\n",
            "Processed 195 images for Henry Cavil\n",
            "Processing images for: Hugh Jackman\n",
            "Processed 179 images for Hugh Jackman\n",
            "Processing images for: Inbar Lavi\n",
            "Processed 127 images for Inbar Lavi\n",
            "Processing images for: Irina Shayk\n",
            "Processed 156 images for Irina Shayk\n",
            "Processing images for: Jake Mcdorman\n",
            "Processed 159 images for Jake Mcdorman\n",
            "Processing images for: Jason Momoa\n",
            "Processed 184 images for Jason Momoa\n",
            "Processing images for: Jennifer Lawrence\n",
            "Processed 180 images for Jennifer Lawrence\n",
            "Processing images for: Jeremy Renner\n",
            "Processed 167 images for Jeremy Renner\n",
            "Processing images for: Jessica Barden\n",
            "Processed 141 images for Jessica Barden\n",
            "Processing images for: Jimmy Fallon\n",
            "Processed 113 images for Jimmy Fallon\n",
            "Processing images for: Johnny Depp\n",
            "Processed 182 images for Johnny Depp\n",
            "Processing images for: Josh Radnor\n",
            "Processed 117 images for Josh Radnor\n",
            "Processing images for: Katharine Mcphee\n",
            "Processed 177 images for Katharine Mcphee\n",
            "Processing images for: Katherine Langford\n",
            "Processed 226 images for Katherine Langford\n",
            "Processing images for: Keanu Reeves\n",
            "Processed 160 images for Keanu Reeves\n",
            "Processing images for: Krysten Ritter\n",
            "Processed 171 images for Krysten Ritter\n",
            "Processing images for: Leonardo DiCaprio\n",
            "Processed 237 images for Leonardo DiCaprio\n",
            "Processing images for: Lili Reinhart\n",
            "Processed 150 images for Lili Reinhart\n",
            "Processing images for: Lindsey Morgan\n",
            "Processed 169 images for Lindsey Morgan\n",
            "Processing images for: Lionel Messi\n",
            "Processed 86 images for Lionel Messi\n",
            "Processing images for: Logan Lerman\n",
            "Processed 212 images for Logan Lerman\n",
            "Processing images for: Madelaine Petsch\n",
            "Processed 191 images for Madelaine Petsch\n",
            "Processing images for: Maisie Williams\n",
            "Processed 193 images for Maisie Williams\n",
            "Processing images for: Maria Pedraza\n",
            "Processed 122 images for Maria Pedraza\n",
            "Processing images for: Marie Avgeropoulos\n",
            "Processed 161 images for Marie Avgeropoulos\n",
            "Processing images for: Mark Ruffalo\n",
            "Processed 178 images for Mark Ruffalo\n",
            "Processing images for: Mark Zuckerberg\n",
            "Processed 95 images for Mark Zuckerberg\n",
            "Processing images for: Megan Fox\n",
            "Processed 209 images for Megan Fox\n",
            "Processing images for: Miley Cyrus\n",
            "Processed 178 images for Miley Cyrus\n",
            "Processing images for: Millie Bobby Brown\n",
            "Processed 191 images for Millie Bobby Brown\n",
            "Processing images for: Morena Baccarin\n",
            "Processed 175 images for Morena Baccarin\n",
            "Processing images for: Morgan Freeman\n",
            "Processed 105 images for Morgan Freeman\n",
            "Processing images for: Nadia Hilker\n",
            "Processed 133 images for Nadia Hilker\n",
            "Processing images for: Natalie Dormer\n",
            "Processed 198 images for Natalie Dormer\n",
            "Processing images for: Natalie Portman\n",
            "Processed 166 images for Natalie Portman\n",
            "Processing images for: Neil Patrick Harris\n",
            "Processed 116 images for Neil Patrick Harris\n",
            "Processing images for: Pedro Alonso\n",
            "Processed 125 images for Pedro Alonso\n",
            "Processing images for: Penn Badgley\n",
            "Processed 171 images for Penn Badgley\n",
            "Processing images for: Rami Malek\n",
            "Processed 160 images for Rami Malek\n",
            "Processing images for: Rebecca Ferguson\n",
            "Processed 178 images for Rebecca Ferguson\n",
            "Processing images for: Richard Harmon\n",
            "Processed 148 images for Richard Harmon\n",
            "Processing images for: Rihanna\n",
            "Processed 133 images for Rihanna\n",
            "Processing images for: Robert De Niro\n",
            "Processed 156 images for Robert De Niro\n",
            "Processing images for: Robert Downey Jr\n",
            "Processed 233 images for Robert Downey Jr\n",
            "Processing images for: Sarah Wayne Callies\n",
            "Processed 159 images for Sarah Wayne Callies\n",
            "Processing images for: Selena Gomez\n",
            "Processed 186 images for Selena Gomez\n",
            "Processing images for: Shakira Isabel Mebarak\n",
            "Processed 154 images for Shakira Isabel Mebarak\n",
            "Processing images for: Sophie Turner\n",
            "Processed 204 images for Sophie Turner\n",
            "Processing images for: Stephen Amell\n",
            "Processed 159 images for Stephen Amell\n",
            "Processing images for: Taylor Swift\n",
            "Processed 131 images for Taylor Swift\n",
            "Processing images for: Tom Cruise\n",
            "Processed 192 images for Tom Cruise\n",
            "Processing images for: Tom Hardy\n",
            "Processed 198 images for Tom Hardy\n",
            "Processing images for: Tom Hiddleston\n",
            "Processed 181 images for Tom Hiddleston\n",
            "Processing images for: Tom Holland\n",
            "Processed 189 images for Tom Holland\n",
            "Processing images for: Tuppence Middleton\n",
            "Processed 133 images for Tuppence Middleton\n",
            "Processing images for: Ursula Corbero\n",
            "Processed 167 images for Ursula Corbero\n",
            "Processing images for: Wentworth Miller\n",
            "Processed 179 images for Wentworth Miller\n",
            "Processing images for: Zac Efron\n",
            "Processed 191 images for Zac Efron\n",
            "Processing images for: Zendaya\n",
            "Processed 138 images for Zendaya\n",
            "Processing images for: Zoe Saldana\n",
            "Processed 186 images for Zoe Saldana\n",
            "Processing images for: alycia dabnem carey\n",
            "Processed 211 images for alycia dabnem carey\n",
            "Processing images for: amber heard\n",
            "Processed 218 images for amber heard\n",
            "Processing images for: barack obama\n",
            "Processed 119 images for barack obama\n",
            "Processing images for: barbara palvin\n",
            "Processed 197 images for barbara palvin\n",
            "Processing images for: camila mendes\n",
            "Processed 162 images for camila mendes\n",
            "Processing images for: elizabeth olsen\n",
            "Processed 221 images for elizabeth olsen\n",
            "Processing images for: ellen page\n",
            "Processed 188 images for ellen page\n",
            "Processing images for: elon musk\n",
            "Processed 135 images for elon musk\n",
            "Processing images for: gal gadot\n",
            "Processed 199 images for gal gadot\n",
            "Processing images for: grant gustin\n",
            "Processed 183 images for grant gustin\n",
            "Processing images for: jeff bezos\n",
            "Processed 106 images for jeff bezos\n",
            "Processing images for: kiernen shipka\n",
            "Processed 203 images for kiernen shipka\n",
            "Processing images for: margot robbie\n",
            "Processed 221 images for margot robbie\n",
            "Processing images for: melissa fumero\n",
            "Processed 154 images for melissa fumero\n",
            "Processing images for: scarlett johansson\n",
            "Processed 201 images for scarlett johansson\n",
            "Processing images for: tom ellis\n",
            "Processed 180 images for tom ellis\n",
            "Data saved to: processed_data/processed_data_20241027_164743.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Create an enhanced ANN model architecture with advanced techniques for better accuracy\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        # Input preprocessing\n",
        "        Flatten(input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "\n",
        "        # First dense block\n",
        "        Dense(2048, kernel_initializer='he_uniform'),\n",
        "        BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        # Second dense block with residual connection\n",
        "        Dense(1024, kernel_initializer='he_uniform'),\n",
        "        BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.4),\n",
        "\n",
        "        # Third dense block\n",
        "        Dense(512, kernel_initializer='he_uniform'),\n",
        "        BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Feature extraction block\n",
        "        Dense(256, kernel_initializer='he_uniform'),\n",
        "        BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        # Deep features block\n",
        "        Dense(128, kernel_initializer='he_uniform'),\n",
        "        BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        # Classification head\n",
        "        Dense(num_classes, activation='softmax',\n",
        "              kernel_initializer='glorot_uniform',\n",
        "              kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "    ])\n",
        "\n",
        "    # Custom learning rate schedule\n",
        "    initial_learning_rate = 0.001\n",
        "    decay_steps = 1000\n",
        "    decay_rate = 0.9\n",
        "    learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate, decay_steps, decay_rate\n",
        "    )\n",
        "\n",
        "    # Custom optimizer with parameters\n",
        "    optimizer = tf.keras.optimizers.AdamW(\n",
        "        learning_rate=learning_rate_schedule,\n",
        "        weight_decay=0.001,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-07\n",
        "    )\n",
        "\n",
        "    # Compile with custom settings\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            tf.keras.metrics.SparseCategoricalAccuracy(name='categorical_accuracy'),\n",
        "            tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_accuracy')\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_model(model, X_train, y_train, X_test, y_test, epochs=50, batch_size=32):\n",
        "    \"\"\"\n",
        "    Enhanced training function with advanced callbacks and monitoring\n",
        "    \"\"\"\n",
        "    # Define initial_learning_rate here\n",
        "    initial_learning_rate = 0.001\n",
        "\n",
        "    # Enhanced data augmentation\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.1,\n",
        "        fill_mode='nearest',\n",
        "        brightness_range=[0.8, 1.2],\n",
        "        preprocessing_function=tf.keras.applications.imagenet_utils.preprocess_input\n",
        "    )\n",
        "\n",
        "    # Advanced callbacks\n",
        "    callbacks = [\n",
        "        # Early stopping with restoration\n",
        "        EarlyStopping(\n",
        "            monitor='val_categorical_accuracy',\n",
        "            patience=15,\n",
        "            restore_best_weights=True,\n",
        "            mode='max'\n",
        "        ),\n",
        "\n",
        "        # Adaptive learning rate reduction\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_categorical_accuracy',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-6,\n",
        "            mode='max',\n",
        "            verbose=1\n",
        "        ),\n",
        "\n",
        "        # Model checkpoint\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            'best_model.keras',  # Change the filepath to end with .keras\n",
        "            monitor='val_categorical_accuracy',\n",
        "            mode='max',\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        " # Train with advanced settings\n",
        "    history = model.fit(\n",
        "        datagen.flow(X_train, y_train, batch_size=batch_size),\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks,\n",
        "        # Removed 'workers' and 'use_multiprocessing' arguments\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    return history, model\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plot training and validation metrics\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax1.plot(history.history['accuracy'])\n",
        "    ax1.plot(history.history['val_accuracy'])\n",
        "    ax1.set_title('Model Accuracy')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend(['Train', 'Validation'])\n",
        "\n",
        "    # Plot loss\n",
        "    ax2.plot(history.history['loss'])\n",
        "    ax2.plot(history.history['val_loss'])\n",
        "    ax2.set_title('Model Loss')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend(['Train', 'Validation'])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "sxTOGFj1H_o7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the preprocessed data\n",
        "X, y, label_map = load_processed_data()\n",
        "\n",
        "print(f\"Dataset summary:\")\n",
        "print(f\"Total number of images: {len(X)}\")\n",
        "print(f\"Number of classes: {len(label_map)}\")\n",
        "print(f\"Images per class:\")\n",
        "for label, name in label_map.items():\n",
        "    count = np.sum(y == label)\n",
        "    print(f\"{name}: {count} images\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")\n",
        "\n",
        "# Create data generator for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Create and train model with enhanced settings\n",
        "model = create_model(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=len(label_map))\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Train with enhanced function\n",
        "history, trained_model = train_model(\n",
        "    model, X_train, y_train, X_test, y_test,\n",
        "    epochs=9,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Evaluate model\n",
        "print(\"\\nEvaluating model...\")\n",
        "test_loss, test_accuracy, cat_accuracy, top_5_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")\n",
        "print(f\"Top-5 accuracy: {top_5_accuracy*100:.2f}%\")\n",
        "\n",
        "\n",
        "# Generate predictions and classification report\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes,\n",
        "                          target_names=[label_map[i] for i in range(len(label_map))]))\n",
        "\n",
        "# Plot training history\n",
        "plot_training_history(history)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(12, 8))\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=list(label_map.values()), yticklabels=list(label_map.values()))\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N3kkhGDoLCpC",
        "outputId": "85590786-dfc8-47bd-a959-00f30aff31a2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from: processed_data/processed_data_20241027_164743.pkl\n",
            "Dataset summary:\n",
            "Total number of images: 17533\n",
            "Number of classes: 105\n",
            "Images per class:\n",
            "Adriana Lima: 213 images\n",
            "Alex Lawther: 152 images\n",
            "Alexandra Daddario: 225 images\n",
            "Alvaro Morte: 139 images\n",
            "Amanda Crew: 117 images\n",
            "Andy Samberg: 196 images\n",
            "Anne Hathaway: 203 images\n",
            "Anthony Mackie: 124 images\n",
            "Avril Lavigne: 162 images\n",
            "Ben Affleck: 126 images\n",
            "Bill Gates: 122 images\n",
            "Bobby Morley: 138 images\n",
            "Brenton Thwaites: 209 images\n",
            "Brian J. Smith: 102 images\n",
            "Brie Larson: 169 images\n",
            "Chris Evans: 166 images\n",
            "Chris Hemsworth: 159 images\n",
            "Chris Pratt: 176 images\n",
            "Christian Bale: 154 images\n",
            "Cristiano Ronaldo: 98 images\n",
            "Danielle Panabaker: 181 images\n",
            "Dominic Purcell: 146 images\n",
            "Dwayne Johnson: 141 images\n",
            "Eliza Taylor: 162 images\n",
            "Elizabeth Lail: 158 images\n",
            "Emilia Clarke: 210 images\n",
            "Emma Stone: 139 images\n",
            "Emma Watson: 211 images\n",
            "Gwyneth Paltrow: 187 images\n",
            "Henry Cavil: 195 images\n",
            "Hugh Jackman: 179 images\n",
            "Inbar Lavi: 127 images\n",
            "Irina Shayk: 156 images\n",
            "Jake Mcdorman: 159 images\n",
            "Jason Momoa: 184 images\n",
            "Jennifer Lawrence: 180 images\n",
            "Jeremy Renner: 167 images\n",
            "Jessica Barden: 141 images\n",
            "Jimmy Fallon: 113 images\n",
            "Johnny Depp: 182 images\n",
            "Josh Radnor: 117 images\n",
            "Katharine Mcphee: 177 images\n",
            "Katherine Langford: 226 images\n",
            "Keanu Reeves: 160 images\n",
            "Krysten Ritter: 171 images\n",
            "Leonardo DiCaprio: 237 images\n",
            "Lili Reinhart: 150 images\n",
            "Lindsey Morgan: 169 images\n",
            "Lionel Messi: 86 images\n",
            "Logan Lerman: 212 images\n",
            "Madelaine Petsch: 191 images\n",
            "Maisie Williams: 193 images\n",
            "Maria Pedraza: 122 images\n",
            "Marie Avgeropoulos: 161 images\n",
            "Mark Ruffalo: 178 images\n",
            "Mark Zuckerberg: 95 images\n",
            "Megan Fox: 209 images\n",
            "Miley Cyrus: 178 images\n",
            "Millie Bobby Brown: 191 images\n",
            "Morena Baccarin: 175 images\n",
            "Morgan Freeman: 105 images\n",
            "Nadia Hilker: 133 images\n",
            "Natalie Dormer: 198 images\n",
            "Natalie Portman: 166 images\n",
            "Neil Patrick Harris: 116 images\n",
            "Pedro Alonso: 125 images\n",
            "Penn Badgley: 171 images\n",
            "Rami Malek: 160 images\n",
            "Rebecca Ferguson: 178 images\n",
            "Richard Harmon: 148 images\n",
            "Rihanna: 133 images\n",
            "Robert De Niro: 156 images\n",
            "Robert Downey Jr: 233 images\n",
            "Sarah Wayne Callies: 159 images\n",
            "Selena Gomez: 186 images\n",
            "Shakira Isabel Mebarak: 154 images\n",
            "Sophie Turner: 204 images\n",
            "Stephen Amell: 159 images\n",
            "Taylor Swift: 131 images\n",
            "Tom Cruise: 192 images\n",
            "Tom Hardy: 198 images\n",
            "Tom Hiddleston: 181 images\n",
            "Tom Holland: 189 images\n",
            "Tuppence Middleton: 133 images\n",
            "Ursula Corbero: 167 images\n",
            "Wentworth Miller: 179 images\n",
            "Zac Efron: 191 images\n",
            "Zendaya: 138 images\n",
            "Zoe Saldana: 186 images\n",
            "alycia dabnem carey: 211 images\n",
            "amber heard: 218 images\n",
            "barack obama: 119 images\n",
            "barbara palvin: 197 images\n",
            "camila mendes: 162 images\n",
            "elizabeth olsen: 221 images\n",
            "ellen page: 188 images\n",
            "elon musk: 135 images\n",
            "gal gadot: 199 images\n",
            "grant gustin: 183 images\n",
            "jeff bezos: 106 images\n",
            "kiernen shipka: 203 images\n",
            "margot robbie: 221 images\n",
            "melissa fumero: 154 images\n",
            "scarlett johansson: 201 images\n",
            "tom ellis: 180 images\n",
            "\n",
            "Training set size: 14026\n",
            "Test set size: 3507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30000\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_6                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30000\u001b[0m)               │         \u001b[38;5;34m120,000\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │      \u001b[38;5;34m61,442,048\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_7                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │           \u001b[38;5;34m8,192\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_5 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │       \u001b[38;5;34m2,098,176\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_8                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │           \u001b[38;5;34m4,096\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_6 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m524,800\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_9                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_7 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m131,328\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_10               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │           \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_8 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_11               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_9 (\u001b[38;5;33mLeakyReLU\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m105\u001b[0m)                 │          \u001b[38;5;34m13,545\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30000</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_6                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30000</span>)               │         <span style=\"color: #00af00; text-decoration-color: #00af00\">120,000</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │      <span style=\"color: #00af00; text-decoration-color: #00af00\">61,442,048</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_7                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,098,176</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_8                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_9                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_10               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_11               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">105</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">13,545</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m64,378,665\u001b[0m (245.59 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,378,665</span> (245.59 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m64,310,729\u001b[0m (245.33 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,310,729</span> (245.33 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m67,936\u001b[0m (265.38 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">67,936</span> (265.38 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m437/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.0101 - categorical_accuracy: 0.0101 - loss: 5.5176 - top_5_accuracy: 0.0534\n",
            "Epoch 1: val_categorical_accuracy improved from -inf to 0.00656, saving model to best_model.keras\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 801ms/step - accuracy: 0.0101 - categorical_accuracy: 0.0101 - loss: 5.5155 - top_5_accuracy: 0.0534 - val_accuracy: 0.0066 - val_categorical_accuracy: 0.0066 - val_loss: 19188.3828 - val_top_5_accuracy: 1.0000 - learning_rate: 9.5480e-04\n",
            "Epoch 2/9\n",
            "\u001b[1m438/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.0109 - categorical_accuracy: 0.0109 - loss: 4.7849 - top_5_accuracy: 0.0548\n",
            "Epoch 2: val_categorical_accuracy improved from 0.00656 to 0.00912, saving model to best_model.keras\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m358s\u001b[0m 816ms/step - accuracy: 0.0109 - categorical_accuracy: 0.0109 - loss: 4.7847 - top_5_accuracy: 0.0548 - val_accuracy: 0.0091 - val_categorical_accuracy: 0.0091 - val_loss: 32994.5898 - val_top_5_accuracy: 1.0000 - learning_rate: 9.1164e-04\n",
            "Epoch 3/9\n",
            "\u001b[1m438/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.0105 - categorical_accuracy: 0.0105 - loss: 4.6859 - top_5_accuracy: 0.0556\n",
            "Epoch 3: val_categorical_accuracy improved from 0.00912 to 0.01084, saving model to best_model.keras\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m413s\u001b[0m 885ms/step - accuracy: 0.0105 - categorical_accuracy: 0.0105 - loss: 4.6858 - top_5_accuracy: 0.0556 - val_accuracy: 0.0108 - val_categorical_accuracy: 0.0108 - val_loss: 60562.5430 - val_top_5_accuracy: 1.0000 - learning_rate: 8.7044e-04\n",
            "Epoch 4/9\n",
            "\u001b[1m438/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.0113 - categorical_accuracy: 0.0113 - loss: 4.6673 - top_5_accuracy: 0.0567\n",
            "Epoch 4: val_categorical_accuracy did not improve from 0.01084\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 115ms/step - accuracy: 0.0113 - categorical_accuracy: 0.0113 - loss: 4.6673 - top_5_accuracy: 0.0567 - val_accuracy: 0.0097 - val_categorical_accuracy: 0.0097 - val_loss: 32083.4688 - val_top_5_accuracy: 1.0000 - learning_rate: 8.3109e-04\n",
            "Epoch 5/9\n",
            "\u001b[1m438/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.0124 - categorical_accuracy: 0.0124 - loss: 4.6566 - top_5_accuracy: 0.0561\n",
            "Epoch 5: val_categorical_accuracy did not improve from 0.01084\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 115ms/step - accuracy: 0.0124 - categorical_accuracy: 0.0124 - loss: 4.6566 - top_5_accuracy: 0.0561 - val_accuracy: 0.0080 - val_categorical_accuracy: 0.0080 - val_loss: 31730.7070 - val_top_5_accuracy: 1.0000 - learning_rate: 7.9353e-04\n",
            "Epoch 6/9\n",
            "\u001b[1m438/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.0120 - categorical_accuracy: 0.0120 - loss: 4.6575 - top_5_accuracy: 0.0558\n",
            "Epoch 6: val_categorical_accuracy did not improve from 0.01084\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 115ms/step - accuracy: 0.0120 - categorical_accuracy: 0.0120 - loss: 4.6575 - top_5_accuracy: 0.0558 - val_accuracy: 0.0106 - val_categorical_accuracy: 0.0106 - val_loss: 31215.3750 - val_top_5_accuracy: 1.0000 - learning_rate: 7.5766e-04\n",
            "Epoch 7/9\n",
            "\u001b[1m438/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.0138 - categorical_accuracy: 0.0138 - loss: 4.6478 - top_5_accuracy: 0.0618\n",
            "Epoch 7: val_categorical_accuracy did not improve from 0.01084\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 116ms/step - accuracy: 0.0138 - categorical_accuracy: 0.0138 - loss: 4.6478 - top_5_accuracy: 0.0617 - val_accuracy: 0.0094 - val_categorical_accuracy: 0.0094 - val_loss: 24712.5859 - val_top_5_accuracy: 1.0000 - learning_rate: 7.2341e-04\n",
            "Epoch 8/9\n",
            "\u001b[1m438/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.0139 - categorical_accuracy: 0.0139 - loss: 4.6469 - top_5_accuracy: 0.0627"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "This optimizer was created with a `LearningRateSchedule` object as its `learning_rate` constructor argument, hence its learning rate is not settable. If you need the learning rate to be settable, you should instantiate the optimizer with a float `learning_rate` argument.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7e7e45d1376d>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Train with enhanced function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m history, trained_model = train_model(\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-0bf207a3c2b1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, X_train, y_train, X_test, y_test, epochs, batch_size)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m  \u001b[0;31m# Train with advanced settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36mlearning_rate\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m    528\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_schedule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLearningRateSchedule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             ):\n\u001b[0;32m--> 530\u001b[0;31m                 raise TypeError(\n\u001b[0m\u001b[1;32m    531\u001b[0m                     \u001b[0;34m\"This optimizer was created with a `LearningRateSchedule`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                     \u001b[0;34m\" object as its `learning_rate` constructor argument, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: This optimizer was created with a `LearningRateSchedule` object as its `learning_rate` constructor argument, hence its learning rate is not settable. If you need the learning rate to be settable, you should instantiate the optimizer with a float `learning_rate` argument."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c0O5On58d3dK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}